""" SCVUR v2 16S alternate metabarcode pipeline, part 1

Metabarcode pipeline to process Illumina MiSeq reads run by run when memory requirements become a problem with the 32-bit USEARCH unoise3 denoising:

1_Pair reads
2_Trim reads
3_Concatenate samples for global analysis
4_Dereplicate reads
5_Create denoised ESVs

Run this pipeline to denoise reads one run at a time.  Concatenate and dereplicate denoised ESVs.  Concatenate primer trimmed reads.  Then proceed with the alternate metabarcode pipeline, part 2.  See detailed instructions in the README.

"""

#######################################################################
# Read in vars from config_alt.yaml
# Generate new vars, dirs, and filenames
# All code not in rules (this section) is run first before any rules (next section onwards)

# 1_Pair reads
raw_sample_read_wildcards=config["raw_sample_read_wildcards"]
print(raw_sample_read_wildcards)

SAMPLES,READS = glob_wildcards(raw_sample_read_wildcards)
SAMPLES_UNIQUE = list(set(SAMPLES))

raw_sample_forward_wildcard=config["raw_sample_forward_wildcard"]

dir=config["dir"]
R1stats_out="{sample}.R1stats"
raw1_stats_out=dir+"/stats/"+R1stats_out
cat_raw1_stats_out=dir+"/stats/R1.stats"

raw_sample_reverse_wildcard=config["raw_sample_reverse_wildcard"]
R2stats_out="{sample}.R2stats"
raw2_stats_out=dir+"/stats/"+R2stats_out
cat_raw2_stats_out=dir+"/stats/R2.stats"

fastq_gz="{sample}.fastq.gz"
seqprep_out=dir+"/paired/"+fastq_gz
Pstats_out="{sample}.Pstats"
paired_stats_out=dir+"/stats/"+Pstats_out
cat_paired_stats_out=dir+"/stats/paired.stats"

# 2_Trim reads
cutadapt_f_out=dir+"/Fprimer_trimmed/"+fastq_gz
Fstats_out="{sample}.Fstats"
Ftrimmed_stats_out=dir+"/stats/"+Fstats_out
cat_Ftrimmed_stats_out=dir+"/stats/Ftrimmed.stats"

fasta_gz="{sample}.fasta.gz"
cutadapt_r_out=dir+"/Rprimer_trimmed/"+fasta_gz
Rstats_out="{sample}.Rstats"
Rtrimmed_stats_out=dir+"/stats/"+Rstats_out
cat_Rtrimmed_stats_out=dir+"/stats/Rtrimmed.stats"

# 3_Concatenate samples for global analysis
fasta="{sample}.fasta"
concatenate_pattern=dir+"/"+fasta
output=dir+"/cat.fasta"
output2=dir+"/cat.fasta2"
gzip_out=dir+"/cat.fasta2.gz"

# 4_Dereplicate reads
vsearch_out=dir+"/cat.uniques"
vsearch_log=dir+"/dereplication.log"

# 5_Create denoised ESVs
usearch_out=dir+"/cat.denoised"
usearch_log=dir+"/usearch.log"

#######################################################################
# This rule defines the final target file that is needed from the pipeline
# Hashed lines (begin with # symbol) are not executed
# Target files need to be separated with a comma
# Snakemake looks at all input and output files to determine which rules need to be run to create the target file

rule all:
	input:
		# Rule testing [In order of execution]:
		# Calculate raw stats
		expand(raw1_stats_out, sample=SAMPLES_UNIQUE),
		expand(raw2_stats_out, sample=SAMPLES_UNIQUE),
		cat_raw1_stats_out,
		cat_raw2_stats_out,
		# 1_Pair reads
#		expand(seqprep_out, sample=SAMPLES)
		# Calculate paired stats
		expand(paired_stats_out, sample=SAMPLES_UNIQUE),
		cat_paired_stats_out,
		# 2_Trim reads (forward)
#		expand(cutadapt_f_out, sample=SAMPLES)
		# Calculate forward trimmed stats
		expand(Ftrimmed_stats_out, sample=SAMPLES_UNIQUE),
		cat_Ftrimmed_stats_out,
		# 2_Trim reads (reverse)
#		expand(cutadapt_r_out, sample=SAMPLES)
		# Calculate reverse trimmed stats
		expand(Rtrimmed_stats_out, sample=SAMPLES_UNIQUE),
		cat_Rtrimmed_stats_out,
		# 2_Trim reads (edit fasta header)
#		expand(concatenate_pattern, sample=SAMPLES_UNIQUE)
		# 3_Concatenate samples for global analysis
#		output
		# 3_Concatenate samples for global analysis (edit fasta header)
#		output2
		# 3_Concatenate samples for global analysis (compress file)
#		gzip_out
		# 4_Dereplicate reads
#		vsearch_out
		# 5_Denoise reads
		usearch_out
	 
#######################################################################
# Count raw forward reads
# For each file calculates number of reads (total), length of reads (min, max, mean, median, mode)
# To summarize output, go to directory and enter 'cat *.stats' > R1.stats'

rule raw1_stats:
	input: 
		raw_sample_forward_wildcard
	output:
		raw1_stats_out
	shell:
		"perl perl_scripts/fastq_gz_stats.plx {input} >> {output}"

#######################################################################
# Concatenate R1 reads

rule cat_raw1_stats:
	input:
		expand(raw1_stats_out, sample=SAMPLES_UNIQUE)
	output:
		cat_raw1_stats_out
	shell:
		"cat {input} >> {output}"

#######################################################################
# Count raw reverse reads
# For each file calculates number of reads (total), length of reads (min, max, mean, median, mode)
# To summarize output, go to directory and enter 'cat *.stats' > R2.stats'

rule raw2_stats:
	input: 
		raw_sample_reverse_wildcard
	output:
		raw2_stats_out
	shell:
		"perl perl_scripts/fastq_gz_stats.plx {input} >> {output}"

#######################################################################
# Concatenate R2 reads

rule cat_raw2_stats:
	input:
		expand(raw2_stats_out, sample=SAMPLES_UNIQUE)
	output:
		cat_raw2_stats_out
	shell:
		"cat {input} >> {output}"

#######################################################################
# Pair forward and reverse reads with SeqPrep

rule pair_reads:
	version: "1.3.2"
	input:
		f=raw_sample_forward_wildcard,
		r=raw_sample_reverse_wildcard
	output:
		X1=temp("{sample}_R1.out"),
		X2=temp("{sample}_R2.out"),
		s=seqprep_out
	shell:
		"SeqPrep -f {input.f} -r {input.r} -1 {output.X1} -2 {output.X2} -q {config[SEQPREP][q]} -s {output.s} -o {config[SEQPREP][o]}"

#######################################################################
# Count paired reads
# For each file calculates number of reads (total), length of reads (min, max, mean, median, mode)
# To summarize output, go to directory and enter 'cat *.stats' > paired.stats'

rule paired_stats:
	input: 
		seqprep_out
	output:
		paired_stats_out
	shell:
		"perl perl_scripts/fastq_gz_stats.plx {input} >> {output}"

#######################################################################
# Concatenate paired reads

rule cat_paired_stats:
	input:
		expand(paired_stats_out, sample=SAMPLES_UNIQUE)
	output:
		cat_paired_stats_out
	shell:
		"cat {input} >> {output}"

#######################################################################
# Trim forward primer with CUTADAPT

rule trim_forward_primer:
	version: "2.4"
	input:
		seqprep_out
	output:
		cutadapt_f_out
	shell:
		"cutadapt -g {config[CUTADAPT_FWD][g]} -m {config[CUTADAPT_FWD][m]} -q {config[CUTADAPT_FWD][q]} --max-n={config[CUTADAPT_FWD][mn]} --discard-untrimmed -o {output} {input}"

#######################################################################
# Count forward primer trimmed reads
# For each file calculates number of reads (total), length of reads (min, max, mean, median, mode)
# To summarize output, go to directory and enter 'cat *.stats' > Ftrimmed.stats'

rule Ftrimmed_stats:
	input: 
		cutadapt_f_out
	output:
		Ftrimmed_stats_out
	shell:
		"perl perl_scripts/fastq_gz_stats.plx {input} >> {output}"

#######################################################################
# Concatenate Ftrimmed reads

rule cat_Ftrimmed_stats:
	input:
		expand(Ftrimmed_stats_out, sample=SAMPLES_UNIQUE)
	output:
		cat_Ftrimmed_stats_out
	shell:
		"cat {input} >> {output}"

#######################################################################
# Trim reverse primer with CUTADAPT

rule trim_reverse_primer:
	version: "2.4"
	input:
		cutadapt_f_out
	output:
		cutadapt_r_out
	shell:
		"cutadapt -a {config[CUTADAPT_REV][a]} -m {config[CUTADAPT_REV][m]} -q {config[CUTADAPT_REV][q]} --max-n={config[CUTADAPT_REV][mn]} --discard-untrimmed -o {output} {input}"

#######################################################################
# Count reverse primer trimmed reads
# For each file calculates number of reads (total), length of reads (min, max, mean, median, mode)
# To summarize output, go to directory and enter 'cat *.stats' > Rtrimmed.stats'

rule Rtrimmed_stats:
	input: 
		cutadapt_r_out
	output:
		Rtrimmed_stats_out
	shell:
		"perl perl_scripts/fasta_gz_stats.plx {input} >> {output}"

#######################################################################
# Concatenate Rtrimmed reads

rule cat_Rtrimmed_stats:
	input:
		expand(Rtrimmed_stats_out, sample=SAMPLES_UNIQUE)
	output:
		cat_Rtrimmed_stats_out
	shell:
		"cat {input} >> {output}"

#######################################################################
# Edit fasta header with Perl script

rule edit_fasta_header1:
	input:
		cutadapt_r_out
	output:
		concatenate_pattern
	threads: 1
	shell:
		"perl perl_scripts/rename_fasta_gzip.plx {input} > {output}"

#######################################################################
# Concatenate all samples into single file for global analysis using Linux cat
# Only use a single thread to work right

rule concatenate:
	input:
		expand(concatenate_pattern, sample=SAMPLES_UNIQUE)
	output:
		temp(output)
	threads: 1
	shell:
		"cat {input} >> {output}"

#######################################################################
# Edit fasta header again using GNU sed

rule edit_fasta_header2:
	version: "4.7"
	input:
		output
	output:
		temp(output2)
	shell:
		"sed -e 's/-/_/g' {input} >> {output}"

#######################################################################
# Compress into smaller file using Linux gzip

rule compress:
	version: "1.7"
	input:
		output2
	output:
		gzip_out
	shell:
		"gzip -c {input} > {output}"

#######################################################################
# Dereplicate and track reads with VSEARCH

rule dereplicate:
	version: "2.13.6"
	input:
		gzip_out
	output:
		vsearch_out
	log: vsearch_log
	shell:
		"vsearch --derep_fulllength {input} --output {output} --sizein --sizeout --log {log}"
	
#######################################################################
# Denoise with USEARCH
# Make sure this is installed locally and in your PATH
# I changed the default name of the program to 'usearch11' to be more concise

rule denoise:
	version: "11.0.667_i86linux32"
	input:
		vsearch_out
	output:
		usearch_out
	log: usearch_log
	shell:
		"usearch11 -unoise3 {input} -zotus {output} -minsize {config[USEARCH][minsize]} > {log} 2>&1"
	
# End of alternate metabarcode pipeline, part 1
